{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10577758,"sourceType":"datasetVersion","datasetId":6545976},{"sourceId":10585798,"sourceType":"datasetVersion","datasetId":6551186},{"sourceId":10586031,"sourceType":"datasetVersion","datasetId":6551358}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kishladwivedi2/send-time-slots?scriptVersionId=221166048\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **Send_Time_Slots**","metadata":{}},{"cell_type":"markdown","source":"Due to the large size of the dataset, I was unable to complete the task within a single Kaggle session. As a result, I worked across multiple files to ensure the process could be continued seamlessly. Since I can only upload one notebook, I have included the code from the other notebooks in a single block for clarity. Additionally, I have provided a Google Drive link for the part that could not be displayed directly within the notebook.\n\nDrive link:- https://drive.google.com/drive/folders/1WGJxOGPaX8nQPgI_pDd845GG6vZuK_um?usp=sharing\n\nThank you for your understanding.","metadata":{}},{"cell_type":"code","source":"# # -*- coding: utf-8 -*-\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# df1 = pd.read_csv(\"/kaggle/input/12345678/train_action_history.csv\")\n# df2 = pd.read_csv(\"/kaggle/input/12345678/train_cdna_data.csv\")\n\n# df2 = df2.loc[:, df2.nunique() > 1]\n\n# df2.shape\n\n# threshold = 0.90\n# df2 = df2.loc[:, df2.isna().mean() < threshold]\n\n# df2.shape\n\n# pd.set_option('display.max_columns', None)\n\n# df2.head(10)\n\n# pd.set_option('display.max_rows', None)\n\n# unique_counts = df2['v302'].value_counts()\n# unique_counts\n\n# # List of columns to drop\n# columns_to_drop = [\n#     'v6', 'v9', 'v33', 'v34', 'v35', 'v36', 'v37', 'v27', 'v49', 'v50', 'v55',\n#     'v56', 'v61', 'v64', 'v69', 'v73', 'v79', 'v84', 'v86', 'v89', 'v90', 'v91',\n#     'v96', 'v97', 'v98', 'v10', 'v31', 'v100', 'v134', 'v135', 'v136', 'v137',\n#     'v146', 'v171', 'v272'\n# ]\n\n# # Drop the specified columns\n# df2 = df2.drop(columns=columns_to_drop, errors='ignore')\n\n\n# # Print the first few rows to verify\n# # df2.head(10)\n\n# df2.shape\n\n# df2.isna().sum()\n\n# # unique_counts = df2['v113'].value_counts()\n# # unique_counts\n\n# df2['v101'] = df2['v101'].replace('Rural', 'Tier 0')\n\n# df2['v58'] = pd.to_numeric(df2['v58'].replace('N', np.nan), errors='coerce')\n\n# df2['v54'] = df2['v54'].str.lower()  # Convert all to lowercase\n\n# # Keep only 'male' or 'female', replace others with NaN\n# df2['v54'] = df2['v54'].replace({\n#     'male': 'Male',\n#     'female': 'Female'\n# })\n\n# # Replace any value that is not 'Male' or 'Female' with NaN\n# df2['v54'] = df2['v54'].apply(lambda x: x if x in ['Male', 'Female'] else np.nan)\n\n# df2['v30'] = pd.to_numeric(df2['v30'].replace('false', np.nan), errors='coerce')\n\n# df2['v7'] = pd.to_numeric(df2['v7'].replace('ZZ', np.nan), errors='coerce')\n\n# df2['v5'] = pd.to_numeric(df2['v5'].replace('ZZ', np.nan), errors='coerce')\n\n# df2.head()\n\n# df2 = df2.drop(columns='V5', errors='ignore')\n\n# threshold = 0.80\n# df2 = df2.loc[:, df2.isna().mean() < threshold]\n\n# df2.shape\n\n# df2.isnull().sum()\n\n# x = df2.columns\n# x\n\n# df2.dtypes\n\n# unique_counts = df2['v99'].value_counts()\n# unique_counts\n\n# df2.loc[df2['v102'].isin(['Salaried', 'Self_Employed']), 'v102'] = 'Employed'\n\n# # Combine 'Student', 'Minor', 'Homemaker', and 'Retired' into 'Non_Working'\n# df2.loc[df2['v102'].isin(['Student', 'Minor', 'Homemaker', 'Retired']), 'v102'] = 'Non_Working'\n\n# # Combine 'Unemployed' and 'Other' into 'Unemployed_Or_Other'\n# df2.loc[df2['v102'].isin(['Unemployed', 'Other']), 'v102'] = 'Unemployed_Or_Other'\n\n# df2['v286'] = df2['v286'].replace('FRIDAY', 'WEEKDAY')\n\n# df2['v285'] = df2['v285'].replace('FRIDAY', 'WEEKDAY')\n\n# df2['v284'] = df2['v284'].replace('FRIDAY', 'WEEKDAY')\n\n# df2['v283'] = df2['v283'].replace('FRIDAY', 'WEEKDAY')\n\n# df2['v282'] = df2['v282'].replace('FRIDAY', 'WEEKDAY')\n\n# df2['v281'] = df2['v281'].replace('FRIDAY', 'WEEKDAY')\n\n# df2['v280'] = df2['v280'].replace('FRIDAY', 'WEEKDAY')\n\n# df2['v279'] = df2['v279'].replace('FRIDAY', 'WEEKDAY')\n\n# for column in df2.columns:\n#     try:\n#         # Try filling NaN with the median of the column\n#         df2[column] = df2[column].fillna(df2[column].median())\n#     except Exception as e:\n#         # If an exception occurs, print the column name\n#         print(f\"Error in column {column}\")\n\n# unique_counts = df2['v286'].value_counts()\n# unique_counts\n\n# df2['v68'] = df2['v68'].replace(False, 0)\n\n\n\n# range_mapping = {\n#     '0': '0 to 5L',\n#     '0 to 1L': '0 to 5L',\n#     '100001 to 5L': '0 to 5L',\n#     '5L to 10L': '5L to 3Cr',\n#     '10L to 25L': '5L to 3Cr',\n#     '25L to 50L': '5L to 3Cr',\n#     '50L to 3Crore': '5L to 3Cr',\n#     '3Crore to 5Crore': 'Above 3Cr',\n#     '5Crore to 10Crore': 'Above 3Cr',\n#     '10Crore to 15Crore': 'Above 3Cr',\n#     '15Crore to 20Crore': 'Above 3Cr',\n#     '20Crore to 35Crore': 'Above 3Cr',\n#     '35Crore to 50Crore': 'Above 3Cr'\n# }\n\n# # Use replace() to map the ranges\n# df2['v63'] = df2['v63'].replace(range_mapping)\n\n# # Define a mapping of categories into three broader groups\n# category_mapping = {\n#     # Loans\n#     'Loans': 'Loans',\n#     'Services and Others': 'Services and Others',\n#     'Production and Trade': 'Loans',\n\n# }\n\n# # Apply the mapping to the dataset\n# df2['v42'] = df2['v42'].map(lambda x: category_mapping.get(x, 'Services and Others'))\n\n# df3=pd.read_csv(\"/kaggle/input/12345678/train_cdna_data.csv\")\n\n# df2['v81'] = df2['v81'].replace({'Y': 1, 'N': 0})\n# df2['v103'] = df2['v103'].astype(int)  # If boolean\n\n# for column in df2.columns:\n#     try:\n#         # Try filling NaN with the median of the column\n#         df2[column] = df2[column].fillna(df2[column].median())\n#     except Exception as e:\n#         # If an exception occurs, print the column name\n#         print(f\"Error in column {column}\", {e})\n\n# # Define mappings\n# morning_evening_map = {'MORNING': 1, 'EVENING': 0}\n# weekend_weekday_map = {'WEEKEND': 1, 'WEEKDAY': 0}\n\n# # Apply mappings\n# df2['v271'] = df2['v271'].map(morning_evening_map)\n# df2['v273'] = df2['v273'].map(morning_evening_map)\n# df2['v274'] = df2['v274'].map(morning_evening_map)\n# df2['v275'] = df2['v275'].map(morning_evening_map)\n# df2['v276'] = df2['v276'].map(morning_evening_map)\n# df2['v277'] = df2['v277'].map(morning_evening_map)\n# df2['v278'] = df2['v278'].map(morning_evening_map)\n# df2['v279'] = df2['v279'].map(weekend_weekday_map)\n# df2['v280'] = df2['v280'].map(weekend_weekday_map)\n# df2['v281'] = df2['v281'].map(weekend_weekday_map)\n# df2['v282'] = df2['v282'].map(weekend_weekday_map)\n# df2['v283'] = df2['v283'].map(weekend_weekday_map)\n# df2['v284'] = df2['v284'].map(weekend_weekday_map)\n# df2['v285'] = df2['v285'].map(weekend_weekday_map)\n# df2['v286'] = df2['v286'].map(weekend_weekday_map)\n\n# df2[column] = df2[column].fillna(df2[column].mode())\n\n\n\n# unique_counts = df2['v99'].value_counts()\n# unique_counts\n\n# # List of columns to drop\n# columns_to_drop = [\n#     'v230', 'v229', 'v228', 'v11', 'v15', 'v29', 'v66', 'v74', 'v102', 'v230', 'grouped_category'\n# ]\n\n# # Drop the specified columns\n# df2 = df2.drop(columns=columns_to_drop, errors='ignore')\n\n\n# # Print the first few rows to verify\n# # df2.head(10)\n\n# lower_bounds = []\n# for value in df2['v2']:\n#     try:\n#         lower_bounds.append(int(value.split('-')[0]))\n#     except (AttributeError, ValueError):  # Handles NaN or invalid formats\n#         lower_bounds.append(np.nan)\n\n# # Add the new column to the DataFrame\n# df2['v2'] = lower_bounds\n\n# # Fill NaN values in 'lower_bound' with the median\n# median_value = df2['v2'].median()\n# df2['v2'].fillna(median_value, inplace=True)\n\n# for col in ['v54', 'v63', 'v99']:\n#     mode_value = df2[col].mode()[0]  # Get the mode of the column\n#     df2[col].fillna(mode_value, inplace=True)\n\n# df2.shape\n\n# df2.head(10)\n\n# unique_counts = df2['v99'].value_counts()\n# unique_counts\n\n# unique_counts = df2['v54'].value_counts()\n# unique_counts\n\n# unique_counts = df2['v63'].value_counts()\n# unique_counts\n\n# unique_counts = df2['v68'].value_counts()\n# unique_counts\n\n# unique_counts = df2['v42'].value_counts()\n# unique_counts\n\n# # First, explicitly fill NaN values with a suitable value (0)\n# df2['v68'] = df2['v68'].fillna('0')\n\n# # Now map the values correctly\n# df2['v68'] = df2['v68'].map({'1.0': 1, '0': 0}).fillna(0)\n\n# # Convert the column to integer type\n# df2['v68'] = df2['v68'].astype(int)\n\n# df2['v42'] = df2['v42'].fillna('Services and Others')\n\n# # Map the values to 0 and 1\n# df2['v42'] = df2['v42'].map({'Services and Others': 0, 'Loans': 1})\n\n# # Now, convert the column to integer type\n# df2['v42'] = df2['v42'].astype(int)\n\n# df2['v54'] = df2['v54'].map({'Male': 0, 'Female': 1}).astype(int)\n\n# # Mapping for 'v99' (and -> 0, ios -> 1)\n# df2['v99'] = df2['v99'].map({'and': 0, 'ios': 1}).astype(int)\n\n# df2['v63'] = df2['v63'].replace('Above 3Cr', '5L to 3Cr')\n\n# # Mapping for 'v63' (0 to 5L -> 0, 5L to 3Cr -> 1)\n# df2['v63'] = df2['v63'].map({'0 to 5L': 0, '5L to 3Cr': 1}).astype(int)\n\n# # Convert boolean columns to integers\n# df2['v45'] = df2['v45'].astype(int)\n# df2['v51'] = df2['v51'].astype(int)\n# df2['v52'] = df2['v52'].astype(int)\n# df2['v53'] = df2['v53'].astype(int)\n\n# C_code = df2['CUSTOMER_CODE']\n# B_date = df2['batch_date']\n\n# # List of columns to drop\n# columns_to_drop = [\n#     'CUSTOMER_CODE', 'batch_date'\n# ]\n\n# # Drop the specified columns\n# df2 = df2.drop(columns=columns_to_drop, errors='ignore')\n\n\n# # Print the first few rows to verify\n# # df2.head(10)\n\n# df2.shape\n\n# df2.head()\n\n# df2.to_csv('Nan_Filled.csv', index=False)\n\n# new_df = pd.DataFrame({\n#     'CUSTOMER_CODE': C_code,\n#     'batch_date': B_date\n# })\n\n# new_df.head(10)\n\n# new_df.to_csv(\"Customer_Code1.csv\", index = False)\n\n# # -*- coding: utf-8 -*-\n\n# df = pd.read_csv('/kaggle/input/cleared-dataset/Nan_Filled.csv')\n\n# df.shape\n\n# df.head()\n\n# pd.set_option('display.max_columns', None)\n\n# df2=pd.read_csv('/kaggle/input/customer-code/Customer_Code1.csv')\n\n# df2.shape\n\n# df2.head()\n\n# from sklearn.feature_selection import VarianceThreshold\n\n# # Define a variance threshold (e.g., 0.01 for very low variance)\n# threshold = 0.01\n\n# # Apply VarianceThreshold\n# selector = VarianceThreshold(threshold=threshold)\n# df_reduced = df.loc[:, selector.fit(df).get_support()]\n\n# # Display the result\n# print(f\"Columns removed: {list(set(df.columns) - set(df_reduced.columns))}\")\n\n# df_reduced.shape\n\n# correlation_threshold = 0.9\n\n# # Calculate the correlation matrix\n# correlation_matrix = df_reduced.corr()\n\n# # Find columns to drop\n# # Select the upper triangle of the correlation matrix\n# upper_triangle = correlation_matrix.where(\n#     np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n# )\n\n# # Find columns with correlation above the threshold\n# to_drop = [\n#     column for column in upper_triangle.columns if any(upper_triangle[column] > correlation_threshold)\n# ]\n\n# # Drop the highly correlated columns\n# df_reduced = df_reduced.drop(columns=to_drop)\n\n# df_reduced.shape\n\n# df_reduced.describe()\n\n# binary_columns = df_reduced.select_dtypes(include=['int', 'float']).columns\n# continuous_columns = df_reduced.select_dtypes(exclude=['int', 'float']).columns\n\n# pd.set_option('display.max_rows', None)\n\n# unique_values_count = df_reduced.nunique()\n# unique_values_count\n\n# import pandas as pd\n# from sklearn.decomposition import PCA\n# from sklearn.preprocessing import StandardScaler\n\n# # List of columns to apply PCA on\n# pca_columns = ['v2', 'v5', 'v7', 'v30', 'v32', 'v77', 'v110', 'v116', 'v128',\n#                'v172', 'v174', 'v176', 'v177', 'v178', 'v203', 'v204', 'v206',\n#                'v207', 'v208', 'v209', 'v210', 'v211', 'v213', 'v215', 'v216',\n#                'v224', 'v225', 'v227', 'v238', 'v239', 'v241', 'v243', 'v244',\n#                'v255', 'v257', 'v258', 'v259', 'v263']\n\n# # Selecting the specified columns from the DataFrame for PCA\n# df_selected = df_reduced[pca_columns]\n\n# # Standardizing the selected data\n# scaler = StandardScaler()\n# scaled_data = scaler.fit_transform(df_selected)\n\n# # Applying PCA\n# pca = PCA()\n# pca_data = pca.fit_transform(scaled_data)\n\n# # Calculating the explained variance ratio\n# explained_variance = pca.explained_variance_ratio_\n\n# # Finding the number of components that capture 95% of the variance\n# cumulative_variance = explained_variance.cumsum()\n# num_components = (cumulative_variance <= 0.95).sum() + 1\n\n# # Applying PCA with the selected number of components\n# pca_final = PCA(n_components=num_components)\n# pca_reduced_data = pca_final.fit_transform(scaled_data)\n\n# # Creating a DataFrame for the reduced data\n# pca_columns = [f'PC{i+1}' for i in range(num_components)]\n# pca_df = pd.DataFrame(pca_reduced_data, columns=pca_columns)\n\n# # Recombining the PCA-reduced data with the original DataFrame (excluding PCA columns)\n# # First drop only the columns used in PCA transformation\n# df_non_pca = df_reduced.drop(columns=pca_columns, errors='ignore')\n\n# # Concatenate the reduced PCA data with the original (non-PCA) columns\n# df_final = pd.concat([df_non_pca, pca_df], axis=1)\n\n# # Show the final DataFrame with PCA applied\n# # print(df_final)\n\n\n\n# # List of columns to drop\n# columns_to_drop = ['v2', 'v5', 'v7', 'v30', 'v32', 'v77', 'v110', 'v116', 'v128',\n#                'v172', 'v174', 'v176', 'v177', 'v178', 'v203', 'v204', 'v206',\n#                'v207', 'v208', 'v209', 'v210', 'v211', 'v213', 'v215', 'v216',\n#                'v224', 'v225', 'v227', 'v238', 'v239', 'v241', 'v243', 'v244',\n#                'v255', 'v257', 'v258', 'v259', 'v263']\n\n# # Drop the specified columns\n# df_final = df_final.drop(columns=columns_to_drop, errors='ignore')\n\n# df_final.head(10)\n\n# merged_df = pd.concat([df2, df_final], axis=1)\n\n# merged_df.shape\n\n# merged_df.head()\n\n# merged_df['batch_date'] = pd.to_datetime(merged_df['batch_date'])\n\n# # Extracting the month from the 'batch_date' column\n# merged_df['batch_month'] = merged_df['batch_date'].dt.month\n\n# df3 = pd.read_csv('/kaggle/input/12345678/train_action_history.csv')\n\n# df3.head()\n\n# import pandas as pd\n# import numpy as np\n\n\n\n\n\n# # Ensure timestamps are converted to pandas datetime objects\n# if 'send_timestamp' in df3.columns:\n#     df3['send_timestamp'] = pd.to_datetime(df3['send_timestamp'], errors='coerce', format='%Y-%m-%dT%H:%M:%S.%fZ')\n# if 'open_timestamp' in df3.columns:\n#     df3['open_timestamp'] = pd.to_datetime(df3['open_timestamp'], errors='coerce', format='%Y-%m-%dT%H:%M:%S.%fZ')\n\n# # Define a function to calculate slots based on vectorized pandas operations\n# def calculate_slots(timestamps):\n#     \"\"\"\n#     Vectorized function to calculate slots (1-28) for given timestamps.\n#     \"\"\"\n#     # Weekday: Monday=0, Sunday=6\n#     day_of_week = timestamps.dt.weekday\n\n#     # Hour: Extract hour from timestamp\n#     hour = timestamps.dt.hour\n\n#     # Map hours to slots within a day\n#     slot_within_day = np.select(\n#         condlist=[\n#             (hour >= 9) & (hour < 12),\n#             (hour >= 12) & (hour < 15),\n#             (hour >= 15) & (hour < 18),\n#             (hour >= 18) & (hour < 21),\n#         ],\n#         choicelist=[1, 2, 3, 4],\n#         default=np.nan  # Default is NaN for out-of-range hours\n#     )\n\n#     # Calculate the overall slot (1–28)\n#     overall_slot = (day_of_week * 4 + slot_within_day).astype('Int64')  # Convert to integer\n#     return overall_slot\n\n# # Calculate slots for 'send_timestamp' and 'open_timestamp'\n# if 'send_timestamp' in df3.columns:\n#     df3['send_slot'] = calculate_slots(df3['send_timestamp'])\n\n# if 'open_timestamp' in df3.columns:\n#     df3['open_slot'] = calculate_slots(df3['open_timestamp'])\n\n# # Display the updated DataFrame\n# (df3.head(10))\n\n# # Convert send_timestamp to datetime and extract the month\n# df3['send_month'] = pd.to_datetime(df3['send_timestamp']).dt.month\n\n# df3.head()\n\n# df3.shape\n\n# merged_df.shape\n\n# df3.head(10)\n\n# merged_df.head(10)\n\n# result_df = pd.merge(df3, merged_df, left_on=['customer_code', 'send_month'], right_on=['CUSTOMER_CODE', 'batch_month'])\n\n# result_df.head(10)\n\n# result_df.shape\n\n# # List of columns to drop\n# columns_to_drop = [\n#     'CUSTOMER_CODE', 'batch_date', 'batch_month', 'send_timestamp', 'open_timestamp', 'send_month', 'Offer_subid', 'batch_id', 'product_category_grouped', 'product_sub_category'\n# ]\n\n# # Drop the specified columns\n# result_df = result_df.drop(columns=columns_to_drop, errors='ignore')\n\n# result_df.head(10)\n\n# unique_values = result_df['product_category'].unique()\n# unique_values\n\n# unique_values = result_df['product_sub_category'].unique()\n# unique_values\n\n# unique_customer_count = result_df['Offer_id'].nunique()\n# unique_customer_count\n\n# unique_customer_count = result_df['Offer_subid'].nunique()\n# unique_customer_count\n\n# unique_customer_count = result_df['batch_id'].nunique()\n# unique_customer_count\n\n# unique_customer_count = result_df['product_category'].nunique()\n# unique_customer_count\n\n# unique_customer_count = result_df['product_sub_category'].nunique()\n# unique_customer_count\n\n# print(1)\n\n# # Mapping dictionary\n# category_mapping = {\n#     'CC_ACQ_SECURED': 'Credit Card Services',\n#     'CC_ACQ_UNSECURED': 'Credit Card Services',\n#     'CC_INORGANIC': 'Credit Card Services',\n#     'CC_UPGRADE': 'Credit Card Services',\n#     'CC_SERVICE': 'Credit Card Services',\n#     'CC_SPENDS': 'Credit Card Services',\n#     'CC_ACTIVATION': 'Credit Card Services',\n#     'CC_VAS': 'Credit Card Services',\n#     'CC_REMARKETING': 'Credit Card Services',\n#     'SIP / MF': 'Investment & Wealth Management',\n#     'INSURANCE': 'Investment & Wealth Management',\n#     'NFO': 'Investment & Wealth Management',\n#     'IPO': 'Investment & Wealth Management',\n#     'PIS': 'Investment & Wealth Management',\n#     'LAS': 'Investment & Wealth Management',\n#     'FX': 'Investment & Wealth Management',\n#     'SAVINGS': 'Banking Accounts',\n#     'CURRENT ACCOUNT': 'Banking Accounts',\n#     'DEPOSITS': 'Banking Accounts',\n#     'NRI': 'Banking Accounts',\n#     'ASSETS': 'Loans & Assets',\n#     'PL X-SELL': 'Loans & Assets',\n#     'RURAL': 'Loans & Assets',\n#     'RELATIONSHIP BANKING': 'Relationship & Corporate Banking',\n#     'CORPSAL': 'Relationship & Corporate Banking',\n#     'DEBIT CARD': 'Payment & Transaction Services',\n#     'OPTIMUS': 'Payment & Transaction Services'\n# }\n\n# # Apply the mapping\n# result_df['product_category'] = result_df['product_category'].map(category_mapping)\n\n# result_df.head()\n\n# result_df['Offer_id'] = result_df['Offer_id'].str.replace('AC_1000', '', regex=False).astype(int)\n\n# C_code = result_df['customer_code']\n\n# # List of columns to drop\n# columns_to_drop = [\n#     'customer_code',\n# ]\n\n# # Drop the specified columns\n# result_df = result_df.drop(columns=columns_to_drop, errors='ignore')\n\n# result_df.head()\n\n# one_hot_encoded = pd.get_dummies(result_df['product_category'], prefix='Category')\n\n# one_hot_encoded.columns = one_hot_encoded.columns.str.replace(' ', '_').str.replace('&', 'and')\n\n# result_df = pd.concat([result_df, one_hot_encoded], axis=1)\n\n# result_df.head()\n\n# # List of columns to drop\n# columns_to_drop = [\n#     'product_category',\n# ]\n\n# # Drop the specified columns\n# result_df = result_df.drop(columns=columns_to_drop, errors='ignore')\n\n# result_df['Category_Banking_Accounts'] = result_df['Category_Banking_Accounts'].astype(int)\n# result_df['Category_Credit_Card_Services'] = result_df['Category_Credit_Card_Services'].astype(int)\n# result_df['Category_Investment_and_Wealth_Management'] = result_df['Category_Investment_and_Wealth_Management'].astype(int)\n# result_df['Category_Loans_and_Assets'] = result_df['Category_Loans_and_Assets'].astype(int)\n# result_df['Category_Payment_and_Transaction_Services'] = result_df['Category_Payment_and_Transaction_Services'].astype(int)\n# result_df['Category_Relationship_and_Corporate_Banking'] = result_df['Category_Relationship_and_Corporate_Banking'].astype(int)\n\n# result_df.columns\n\n# new_column_names = {result_df.columns[-6]: 'c1',\n#                     result_df.columns[-5]: 'c2',\n#                     result_df.columns[-4]: 'c3',\n#                     result_df.columns[-3]: 'c4',\n#                     result_df.columns[-2]: 'c5',\n#                     result_df.columns[-1]: 'c6'}\n\n# # Rename columns\n# result_df.rename(columns=new_column_names, inplace=True)\n\n# result_df.head()\n\n# result_df.shape\n\n# result_df.to_csv('FinallyDone.csv', index = False)\n\n\n\n# print(1)\n\n# import pandas as pd\n\n# df = pd.read_csv('/kaggle/working/DataPrepDone.csv')\n\n# pd.set_option('display.max_columns', None)\n\n# df.head()\n\n# df.shape\n\n# df_opened = df[df['open_slot'].notna()]\n# df_closed = df[df['open_slot'].isna()]\n\n# df_opened.shape\n\n# df_closed.shape\n\n# import xgboost as xgb\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score\n# from sklearn.impute import SimpleImputer\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:16:47.276094Z","iopub.execute_input":"2025-01-26T22:16:47.276449Z","iopub.status.idle":"2025-01-26T22:16:47.723127Z","shell.execute_reply.started":"2025-01-26T22:16:47.276415Z","shell.execute_reply":"2025-01-26T22:16:47.721928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:16:47.797176Z","iopub.execute_input":"2025-01-26T22:16:47.797617Z","iopub.status.idle":"2025-01-26T22:16:47.802966Z","shell.execute_reply.started":"2025-01-26T22:16:47.797589Z","shell.execute_reply":"2025-01-26T22:16:47.801843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:16:48.188385Z","iopub.execute_input":"2025-01-26T22:16:48.1888Z","iopub.status.idle":"2025-01-26T22:16:48.194015Z","shell.execute_reply.started":"2025-01-26T22:16:48.188757Z","shell.execute_reply":"2025-01-26T22:16:48.192778Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/traindatacleaned/FinallyDone.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:16:48.811181Z","iopub.execute_input":"2025-01-26T22:16:48.811542Z","iopub.status.idle":"2025-01-26T22:19:03.469281Z","shell.execute_reply.started":"2025-01-26T22:16:48.811493Z","shell.execute_reply":"2025-01-26T22:19:03.468073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:19:32.939497Z","iopub.execute_input":"2025-01-26T22:19:32.940348Z","iopub.status.idle":"2025-01-26T22:19:33.029746Z","shell.execute_reply.started":"2025-01-26T22:19:32.940292Z","shell.execute_reply":"2025-01-26T22:19:33.02854Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming df is your DataFrame\ndf_clean = df['send_slot'].dropna()  # Drop NaN values\n\n# Convert the column to a categorical type to keep the order\ndf_clean = pd.Categorical(df_clean, categories=sorted(df_clean.unique()))\n\n# Create a mapping for the labels\nslot_labels = {val: f'Slot_{int(val)}' for val in df_clean.unique()}\n\n# Plotting the bar graph\nax = df_clean.value_counts().sort_index().plot(kind='bar')\n\n# Manually set the x-axis labels to Slot_1, Slot_2, ...\nax.set_xticklabels([f'Slot_{int(val)}' for val in ax.get_xticks()], rotation=90)\n\nplt.title('Bar Graph for Send Slot')\nplt.xlabel('Send Slot')\nplt.ylabel('Frequency')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:26:03.961066Z","iopub.execute_input":"2025-01-26T22:26:03.961463Z","iopub.status.idle":"2025-01-26T22:26:04.620844Z","shell.execute_reply.started":"2025-01-26T22:26:03.961432Z","shell.execute_reply":"2025-01-26T22:26:04.619725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming df is your DataFrame\ndf_clean = df['open_slot'].dropna()  # Drop NaN values\n\n# Convert the column to a categorical type to keep the order\ndf_clean = pd.Categorical(df_clean, categories=sorted(df_clean.unique()))\n\n# Create a mapping for the labels\nslot_labels = {val: f'Slot_{int(val)}' for val in df_clean.unique()}\n\n# Plotting the bar graph\nax = df_clean.value_counts().sort_index().plot(kind='bar')\n\n# Manually set the x-axis labels to Slot_1, Slot_2, ...\nax.set_xticklabels([f'Slot_{int(val)}' for val in ax.get_xticks()], rotation=90)\n\nplt.title('Bar Graph for Open Slot')\nplt.xlabel('Open Slot')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:25:26.656877Z","iopub.execute_input":"2025-01-26T22:25:26.657271Z","iopub.status.idle":"2025-01-26T22:25:27.167483Z","shell.execute_reply.started":"2025-01-26T22:25:26.657244Z","shell.execute_reply":"2025-01-26T22:25:27.166387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming df is your DataFrame\ncorrelation_matrix = df.corr()  # Calculate the correlation matrix\n\n# Plotting the heatmap\nplt.figure(figsize=(10, 8))  # You can adjust the size as needed\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5, cbar=True)\n\nplt.title('Correlation Heatmap')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:27:28.167239Z","iopub.execute_input":"2025-01-26T22:27:28.167628Z","iopub.status.idle":"2025-01-26T22:29:41.527535Z","shell.execute_reply.started":"2025-01-26T22:27:28.167598Z","shell.execute_reply":"2025-01-26T22:29:41.52641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))  # You can adjust the size as needed\nsns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt='.2f', linewidths=0.5, cbar=True)\n\nplt.title('Correlation Heatmap')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:31:39.844483Z","iopub.execute_input":"2025-01-26T22:31:39.845276Z","iopub.status.idle":"2025-01-26T22:31:40.632298Z","shell.execute_reply.started":"2025-01-26T22:31:39.845231Z","shell.execute_reply":"2025-01-26T22:31:40.631162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.dropna(subset=['send_slot'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:31:49.998781Z","iopub.execute_input":"2025-01-26T22:31:49.999137Z","iopub.status.idle":"2025-01-26T22:31:51.974529Z","shell.execute_reply.started":"2025-01-26T22:31:49.999112Z","shell.execute_reply":"2025-01-26T22:31:51.973425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:31:51.975923Z","iopub.execute_input":"2025-01-26T22:31:51.976213Z","iopub.status.idle":"2025-01-26T22:31:53.364352Z","shell.execute_reply.started":"2025-01-26T22:31:51.976189Z","shell.execute_reply":"2025-01-26T22:31:53.363221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Filter rows where 'open_slot' is NOT NaN\ndf_filtered = df[df['open_slot'].notna()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:31:53.365836Z","iopub.execute_input":"2025-01-26T22:31:53.366175Z","iopub.status.idle":"2025-01-26T22:31:53.992464Z","shell.execute_reply.started":"2025-01-26T22:31:53.366143Z","shell.execute_reply":"2025-01-26T22:31:53.991141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_filtered.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:31:53.993964Z","iopub.execute_input":"2025-01-26T22:31:53.994398Z","iopub.status.idle":"2025-01-26T22:31:54.001145Z","shell.execute_reply.started":"2025-01-26T22:31:53.994357Z","shell.execute_reply":"2025-01-26T22:31:54.00011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_filtered.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:31:54.894522Z","iopub.execute_input":"2025-01-26T22:31:54.894888Z","iopub.status.idle":"2025-01-26T22:31:54.946283Z","shell.execute_reply.started":"2025-01-26T22:31:54.894838Z","shell.execute_reply":"2025-01-26T22:31:54.945024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_filtered['send_slot'] = df_filtered['send_slot'] - 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:31:55.113283Z","iopub.execute_input":"2025-01-26T22:31:55.113701Z","iopub.status.idle":"2025-01-26T22:31:55.12706Z","shell.execute_reply.started":"2025-01-26T22:31:55.113662Z","shell.execute_reply":"2025-01-26T22:31:55.125935Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_filtered['open_slot'] = df_filtered['open_slot'] - 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:32:05.202342Z","iopub.execute_input":"2025-01-26T22:32:05.202712Z","iopub.status.idle":"2025-01-26T22:32:05.214679Z","shell.execute_reply.started":"2025-01-26T22:32:05.202685Z","shell.execute_reply":"2025-01-26T22:32:05.213437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df_filtered.drop(columns=['send_slot', 'open_slot'])  # Features (drop send_slot and open_slot)\ny = df_filtered['send_slot']  # Target variable (send_slot)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:32:06.287184Z","iopub.execute_input":"2025-01-26T22:32:06.287538Z","iopub.status.idle":"2025-01-26T22:32:06.842435Z","shell.execute_reply.started":"2025-01-26T22:32:06.287512Z","shell.execute_reply":"2025-01-26T22:32:06.841179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import xgboost as xgb\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:32:08.936312Z","iopub.execute_input":"2025-01-26T22:32:08.936651Z","iopub.status.idle":"2025-01-26T22:32:09.440836Z","shell.execute_reply.started":"2025-01-26T22:32:08.936625Z","shell.execute_reply":"2025-01-26T22:32:09.439797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tdf1 = pd.read_csv('/kaggle/input/testing-data/TestDataCleaned.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:32:09.442071Z","iopub.execute_input":"2025-01-26T22:32:09.442366Z","iopub.status.idle":"2025-01-26T22:32:13.336589Z","shell.execute_reply.started":"2025-01-26T22:32:09.442341Z","shell.execute_reply":"2025-01-26T22:32:13.335173Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tdf2 = pd.read_csv('/kaggle/input/testing-data/TestcustomerCleaned.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:24:10.580216Z","iopub.execute_input":"2025-01-26T19:24:10.580567Z","iopub.status.idle":"2025-01-26T19:24:11.004283Z","shell.execute_reply.started":"2025-01-26T19:24:10.580531Z","shell.execute_reply":"2025-01-26T19:24:11.003103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tdf1['send_slot'] = tdf1['send_slot'] - 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:24:11.00552Z","iopub.execute_input":"2025-01-26T19:24:11.005884Z","iopub.status.idle":"2025-01-26T19:24:11.013214Z","shell.execute_reply.started":"2025-01-26T19:24:11.005854Z","shell.execute_reply":"2025-01-26T19:24:11.012101Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tdf1['open_slot'] = tdf1['open_slot'] - 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:24:11.014762Z","iopub.execute_input":"2025-01-26T19:24:11.015172Z","iopub.status.idle":"2025-01-26T19:24:11.029727Z","shell.execute_reply.started":"2025-01-26T19:24:11.015134Z","shell.execute_reply":"2025-01-26T19:24:11.028687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df_filtered['send_slot'].unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:24:11.030997Z","iopub.execute_input":"2025-01-26T19:24:11.031611Z","iopub.status.idle":"2025-01-26T19:24:11.064173Z","shell.execute_reply.started":"2025-01-26T19:24:11.031544Z","shell.execute_reply":"2025-01-26T19:24:11.062853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dtrain = xgb.DMatrix(X, label=y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:32:21.651219Z","iopub.execute_input":"2025-01-26T22:32:21.651613Z","iopub.status.idle":"2025-01-26T22:32:25.540314Z","shell.execute_reply.started":"2025-01-26T22:32:21.651571Z","shell.execute_reply":"2025-01-26T22:32:25.539324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# When training the model, use 'multi:softprob' to output probabilities\nparams = {\n    'objective': 'multi:softprob',  # Multi-class classification with probabilities output\n    'num_class': 28,\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'eval_metric': 'merror',\n    'n_estimators': 100,\n}\n\n# Retrain the model with the correct objective\nmodel = xgb.train(params, dtrain, num_boost_round=100)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:32:26.479557Z","iopub.execute_input":"2025-01-26T22:32:26.479959Z","iopub.status.idle":"2025-01-26T22:41:58.73379Z","shell.execute_reply.started":"2025-01-26T22:32:26.479927Z","shell.execute_reply":"2025-01-26T22:41:58.732573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_importance = model.get_score(importance_type='weight')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:03:20.197799Z","iopub.execute_input":"2025-01-26T23:03:20.198158Z","iopub.status.idle":"2025-01-26T23:03:20.223233Z","shell.execute_reply.started":"2025-01-26T23:03:20.19813Z","shell.execute_reply":"2025-01-26T23:03:20.221596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_importance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:03:26.991623Z","iopub.execute_input":"2025-01-26T23:03:26.991986Z","iopub.status.idle":"2025-01-26T23:03:27.000525Z","shell.execute_reply.started":"2025-01-26T23:03:26.991957Z","shell.execute_reply":"2025-01-26T23:03:26.999473Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Data from the dictionary\ndata = {\n    'Offer_id': 38264.0,\n    'v24': 2529.0,\n    'v44': 572.0,\n    'v45': 106.0,\n    'v51': 412.0,\n    'v52': 229.0,\n    'v53': 806.0,\n    'v54': 157.0,\n    'v63': 293.0,\n    'v81': 67.0,\n    'v82': 561.0,\n    'v87': 281.0,\n    'v88': 274.0,\n    'v92': 132.0,\n    'v95': 261.0,\n    'v99': 402.0,\n    'v103': 142.0,\n    'v104': 3380.0,\n    'v105': 133.0,\n    'v106': 1439.0,\n    'v173': 197.0,\n    'v175': 612.0,\n    'v286': 122.0,\n    'v287': 7402.0,\n    'v290': 2596.0,\n    'v291': 1697.0,\n    'v295': 1203.0,\n    'PC1': 2906.0,\n    'PC2': 2264.0,\n    'PC3': 2575.0,\n    'PC4': 1958.0,\n    'PC5': 2051.0,\n    'PC6': 2301.0,\n    'PC7': 2218.0,\n    'PC8': 2459.0,\n    'PC9': 2107.0,\n    'PC10': 2068.0,\n    'PC11': 1957.0,\n    'PC12': 2123.0,\n    'PC13': 2380.0,\n    'PC14': 2392.0,\n}\n\n# Prepare the data for plotting\nlabels = list(data.keys())\nvalues = list(data.values())\n\n# Create the plot\nplt.figure(figsize=(10, 5))\nplt.plot(labels, values, marker='o')\n\n# Customize the plot\nplt.title('Line Graph of Feature Weights from XGBoost')\nplt.xlabel('Featues')\nplt.ylabel('Weights')\nplt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T23:09:27.25152Z","iopub.execute_input":"2025-01-26T23:09:27.251954Z","iopub.status.idle":"2025-01-26T23:09:28.2213Z","shell.execute_reply.started":"2025-01-26T23:09:27.251924Z","shell.execute_reply":"2025-01-26T23:09:28.2202Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming 'model' is an instance of XGBClassifier or XGBRegressor\nbooster = model.get_booster()\nfeature_importance = booster.get_score(importance_type='weight')\n\n# To visualize or further process the feature importance\nimport pandas as pd\n\n# Convert the dictionary to a DataFrame\nimportance_df = pd.DataFrame({\n    'Feature': list(feature_importance.keys()),\n    'Importance': list(feature_importance.values())\n})\n\n# Sort the DataFrame by importance\nimportance_df = importance_df.sort_values(by='Importance', ascending=False)\n\nprint(importance_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tdf1.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:39:08.289877Z","iopub.execute_input":"2025-01-26T19:39:08.290411Z","iopub.status.idle":"2025-01-26T19:39:08.402578Z","shell.execute_reply.started":"2025-01-26T19:39:08.29037Z","shell.execute_reply":"2025-01-26T19:39:08.399668Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test = tdf1.drop(columns=['send_slot', 'open_slot'])  # Features (drop send_slot and open_slot)\ny_test = tdf1['send_slot']  # Target variable (send_slot)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:39:41.295587Z","iopub.execute_input":"2025-01-26T19:39:41.296038Z","iopub.status.idle":"2025-01-26T19:39:41.381944Z","shell.execute_reply.started":"2025-01-26T19:39:41.296009Z","shell.execute_reply":"2025-01-26T19:39:41.381006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dtest = xgb.DMatrix(X_test, label=y_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:39:43.531472Z","iopub.execute_input":"2025-01-26T19:39:43.531843Z","iopub.status.idle":"2025-01-26T19:39:44.131064Z","shell.execute_reply.started":"2025-01-26T19:39:43.531813Z","shell.execute_reply":"2025-01-26T19:39:44.130183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Predict probabilities for all test samples\ny_pred_probs = model.predict(dtest)\n\n# Now y_pred_probs will have the shape (n_samples, 28), where 28 is the number of classes\n# Check if the prediction output is in the expected shape\nif y_pred_probs.shape[1] != 28:\n    raise ValueError(\"Prediction output is not a probability distribution. Check the model or use the correct objective.\")\n\n# Sort the slots by predicted probabilities\nfor i, prob_row in enumerate(y_pred_probs[:10], start=1):\n    sorted_slots = np.argsort(prob_row)[::-1]  # Get indices of slots sorted by probability\n    sorted_slots_with_prob = [(f\"slot{slot + 1}\", prob_row[slot]) for slot in sorted_slots]\n    print(f\"Test Sample {i}: {sorted_slots_with_prob}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:39:45.733281Z","iopub.execute_input":"2025-01-26T19:39:45.733616Z","iopub.status.idle":"2025-01-26T19:40:02.384618Z","shell.execute_reply.started":"2025-01-26T19:39:45.73359Z","shell.execute_reply":"2025-01-26T19:40:02.383314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Create an empty list to store the results\nresults = []\n\n# Iterate through the first 10 test samples\nfor i, prob_row in enumerate(y_pred_probs, start=1):\n    sorted_slots = np.argsort(prob_row)[::-1]  # Get indices of slots sorted by probability\n    sorted_slots_with_prob = [(f\"slot{slot + 1}\", prob_row[slot]) for slot in sorted_slots]\n    \n    # Store the result in the list in the desired format\n    result = {\"Test Sample\": i}\n    for idx, (slot, prob) in enumerate(sorted_slots_with_prob):\n        result[f\"slot_{idx+1}\"] = slot  # Store slot name in decreasing order of probability\n    results.append(result)\n\n# Convert the list of results into a pandas DataFrame\ndf_result = pd.DataFrame(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:40:14.738271Z","iopub.execute_input":"2025-01-26T19:40:14.738648Z","iopub.status.idle":"2025-01-26T19:40:38.459335Z","shell.execute_reply.started":"2025-01-26T19:40:14.738615Z","shell.execute_reply":"2025-01-26T19:40:38.458071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_result.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:40:38.460744Z","iopub.execute_input":"2025-01-26T19:40:38.461261Z","iopub.status.idle":"2025-01-26T19:40:38.469933Z","shell.execute_reply.started":"2025-01-26T19:40:38.46122Z","shell.execute_reply":"2025-01-26T19:40:38.468638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_combined = pd.concat([tdf2, df_result], axis=1) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:40:38.472903Z","iopub.execute_input":"2025-01-26T19:40:38.473386Z","iopub.status.idle":"2025-01-26T19:40:39.023799Z","shell.execute_reply.started":"2025-01-26T19:40:38.473335Z","shell.execute_reply":"2025-01-26T19:40:39.022569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_combined.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:40:39.025912Z","iopub.execute_input":"2025-01-26T19:40:39.026285Z","iopub.status.idle":"2025-01-26T19:40:39.052856Z","shell.execute_reply.started":"2025-01-26T19:40:39.026253Z","shell.execute_reply":"2025-01-26T19:40:39.051442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_combined = df_combined.drop('Test Sample', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:40:39.054076Z","iopub.execute_input":"2025-01-26T19:40:39.054416Z","iopub.status.idle":"2025-01-26T19:40:39.638572Z","shell.execute_reply.started":"2025-01-26T19:40:39.054375Z","shell.execute_reply":"2025-01-26T19:40:39.637346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_combined.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:40:39.639824Z","iopub.execute_input":"2025-01-26T19:40:39.640258Z","iopub.status.idle":"2025-01-26T19:40:39.648116Z","shell.execute_reply.started":"2025-01-26T19:40:39.640217Z","shell.execute_reply":"2025-01-26T19:40:39.646816Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/test-idfc/test_customers.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:40:39.649379Z","iopub.execute_input":"2025-01-26T19:40:39.649845Z","iopub.status.idle":"2025-01-26T19:40:39.75284Z","shell.execute_reply.started":"2025-01-26T19:40:39.649761Z","shell.execute_reply":"2025-01-26T19:40:39.751852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:40:39.754915Z","iopub.execute_input":"2025-01-26T19:40:39.755517Z","iopub.status.idle":"2025-01-26T19:40:39.765132Z","shell.execute_reply.started":"2025-01-26T19:40:39.755463Z","shell.execute_reply":"2025-01-26T19:40:39.763592Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:40:39.766575Z","iopub.execute_input":"2025-01-26T19:40:39.766927Z","iopub.status.idle":"2025-01-26T19:40:39.780892Z","shell.execute_reply.started":"2025-01-26T19:40:39.766882Z","shell.execute_reply":"2025-01-26T19:40:39.779639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_values = df_combined['customer_code'].nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:40:39.782393Z","iopub.execute_input":"2025-01-26T19:40:39.782787Z","iopub.status.idle":"2025-01-26T19:40:39.930072Z","shell.execute_reply.started":"2025-01-26T19:40:39.782742Z","shell.execute_reply":"2025-01-26T19:40:39.929064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T19:40:39.931012Z","iopub.execute_input":"2025-01-26T19:40:39.931287Z","iopub.status.idle":"2025-01-26T19:40:39.937939Z","shell.execute_reply.started":"2025-01-26T19:40:39.931264Z","shell.execute_reply":"2025-01-26T19:40:39.936835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Function to compute the slot priority from multiple slot orders\ndef compute_slot_priority(slot_orders):\n    # Initialize a dictionary to store the cumulative ranking for each slot\n    cumulative_ranking = {f\"slot{i}\": 0 for i in range(1, 29)}\n    \n    # Loop through each row (slot order) to accumulate rankings\n    for order in slot_orders:\n        for rank, slot in enumerate(order, start=1):\n            cumulative_ranking[slot] += rank  # Accumulate the rank\n    \n    # Sort slots based on their cumulative rank (lower sum is higher priority)\n    sorted_slots = sorted(cumulative_ranking.items(), key=lambda x: x[1])\n    \n    # Return the sorted slot names\n    return [slot[0] for slot in sorted_slots]\n\n# Optimized function to compute the priority order\ndef store_priority_order(test, df_combined):\n    # Pre-group `df_combined` by customer_code\n    grouped = df_combined.groupby('customer_code')\n    \n    # Create an empty list to store the priority orders\n    priority_orders = []\n    \n    # Iterate over each customer_code in the test dataframe\n    for idx, row in enumerate(test.itertuples(), start=1):\n        customer_code = row.CUSTOMER_CODE\n        \n        # Get the slot orders for the current customer_code\n        if customer_code in grouped.groups:\n            customer_rows = grouped.get_group(customer_code)\n            slot_orders = customer_rows.drop(columns=['customer_code']).values.tolist()\n            \n            # Compute the priority order\n            priority_order = compute_slot_priority(slot_orders)\n        else:\n            # If no matching customer_code, return an empty priority\n            priority_order = []\n        \n        # Store the priority order as a string\n        priority_orders.append(\", \".join(priority_order))\n        \n        # Print progress every 100 rows\n        if idx % 100 == 0:\n            print(f\"Processed {idx} rows out of {len(test)}...\")\n    \n    # Create a new DataFrame to store the results\n    result_df = pd.DataFrame({\n        'CUSTOMER_CODE': test['CUSTOMER_CODE'],\n        'slot_priority': priority_orders\n    })\n    \n    return result_df\n\n# Assuming `test` and `df_combined` are already defined\nresult_df = store_priority_order(test, df_combined)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:10:57.850724Z","iopub.execute_input":"2025-01-26T20:10:57.851172Z","iopub.status.idle":"2025-01-26T20:11:43.453945Z","shell.execute_reply.started":"2025-01-26T20:10:57.851133Z","shell.execute_reply":"2025-01-26T20:11:43.452568Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:12:29.132461Z","iopub.execute_input":"2025-01-26T20:12:29.13284Z","iopub.status.idle":"2025-01-26T20:12:29.139361Z","shell.execute_reply.started":"2025-01-26T20:12:29.132797Z","shell.execute_reply":"2025-01-26T20:12:29.138125Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_df.replace(['None', 'null', '', ' '], np.nan, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:17:23.386807Z","iopub.execute_input":"2025-01-26T20:17:23.387243Z","iopub.status.idle":"2025-01-26T20:17:23.437546Z","shell.execute_reply.started":"2025-01-26T20:17:23.387213Z","shell.execute_reply":"2025-01-26T20:17:23.436307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_df.isnull().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:17:27.162549Z","iopub.execute_input":"2025-01-26T20:17:27.162898Z","iopub.status.idle":"2025-01-26T20:17:27.18031Z","shell.execute_reply.started":"2025-01-26T20:17:27.16287Z","shell.execute_reply":"2025-01-26T20:17:27.179075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill missing values with the mode for each column\nfor column in result_df.columns:\n    if result_df[column].isnull().any():  # Check if there are missing values\n        mode_value = result_df[column].mode()[0]  # Get the first mode\n        result_df[column].fillna(mode_value, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:17:32.32181Z","iopub.execute_input":"2025-01-26T20:17:32.322226Z","iopub.status.idle":"2025-01-26T20:17:32.38834Z","shell.execute_reply.started":"2025-01-26T20:17:32.32219Z","shell.execute_reply":"2025-01-26T20:17:32.387097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:16:45.209855Z","iopub.execute_input":"2025-01-26T20:16:45.210315Z","iopub.status.idle":"2025-01-26T20:16:45.238753Z","shell.execute_reply.started":"2025-01-26T20:16:45.210283Z","shell.execute_reply":"2025-01-26T20:16:45.237611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:30:50.35597Z","iopub.execute_input":"2025-01-26T20:30:50.356423Z","iopub.status.idle":"2025-01-26T20:30:50.366311Z","shell.execute_reply.started":"2025-01-26T20:30:50.356388Z","shell.execute_reply":"2025-01-26T20:30:50.365165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to format the slot_priority column\ndef format_slots(slot_string):\n    # Split the slot string by commas and strip any extra whitespace\n    slots = [slot.strip() for slot in slot_string.split(\",\")]\n    # Add square brackets around each slot and rejoin them with commas\n    formatted_slots = \", \".join([f\"[{slot}]\" for slot in slots])\n    return formatted_slots\n\n# Apply the function to the slot_priority column\nresult_df['slot_priority'] = result_df['slot_priority'].apply(format_slots)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:18:53.738157Z","iopub.execute_input":"2025-01-26T20:18:53.738533Z","iopub.status.idle":"2025-01-26T20:18:54.332627Z","shell.execute_reply.started":"2025-01-26T20:18:53.738506Z","shell.execute_reply":"2025-01-26T20:18:54.331463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to format the slot_priority column\ndef format_slots_with_underscores(slot_string):\n    # Split the slot string by commas and strip any extra whitespace\n    slots = [slot.strip() for slot in slot_string.split(\",\")]\n    # Ensure each slot is prefixed with 'slot_' and add it to the formatted list\n    formatted_slots = [f\"slot_{slot.replace('slot', '').strip()}\" for slot in slots]\n    # Join the slots into the desired format\n    return f\"[{', '.join(formatted_slots)}]\"\n\n# Apply the function to the slot_priority column\nresult_df['slot_priority'] = result_df['slot_priority'].apply(format_slots_with_underscores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:30:38.189083Z","iopub.execute_input":"2025-01-26T20:30:38.189544Z","iopub.status.idle":"2025-01-26T20:30:39.051221Z","shell.execute_reply.started":"2025-01-26T20:30:38.189513Z","shell.execute_reply":"2025-01-26T20:30:39.050259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to clean and correctly format slot_priority\ndef format_slots_correctly(slot_string):\n    # Split the slot string by commas and strip any extra whitespace\n    slots = [slot.strip() for slot in slot_string.split(\",\")]\n    # Ensure proper formatting like slot_XX without extra brackets\n    formatted_slots = [f\"slot_{slot.replace('slot_', '').replace('[', '').replace(']', '').strip()}\" for slot in slots]\n    # Join the slots into the desired format with square brackets\n    return f\"[{', '.join(formatted_slots)}]\"\n\n# Apply the function to the slot_priority column\nresult_df['slot_priority'] = result_df['slot_priority'].apply(format_slots_correctly)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:20:43.913457Z","iopub.execute_input":"2025-01-26T20:20:43.913892Z","iopub.status.idle":"2025-01-26T20:20:45.233514Z","shell.execute_reply.started":"2025-01-26T20:20:43.913857Z","shell.execute_reply":"2025-01-26T20:20:45.232516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:21:36.192569Z","iopub.execute_input":"2025-01-26T20:21:36.193077Z","iopub.status.idle":"2025-01-26T20:21:36.199731Z","shell.execute_reply.started":"2025-01-26T20:21:36.19303Z","shell.execute_reply":"2025-01-26T20:21:36.19839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"concatenated_df = pd.concat([test, result_df], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:23:59.740148Z","iopub.execute_input":"2025-01-26T20:23:59.740525Z","iopub.status.idle":"2025-01-26T20:23:59.758487Z","shell.execute_reply.started":"2025-01-26T20:23:59.740498Z","shell.execute_reply":"2025-01-26T20:23:59.7573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identify columns with duplicate names\nduplicate_columns = concatenated_df.columns[concatenated_df.columns.duplicated()].tolist()\n\n# Drop the second occurrence of the duplicate column\nif duplicate_columns:\n    concatenated_df = concatenated_df.loc[:, ~concatenated_df.columns.duplicated(keep='first')]\n\n# Display the DataFrame after dropping the second duplicate column\nconcatenated_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:25:26.817272Z","iopub.execute_input":"2025-01-26T20:25:26.817691Z","iopub.status.idle":"2025-01-26T20:25:26.83604Z","shell.execute_reply.started":"2025-01-26T20:25:26.817663Z","shell.execute_reply":"2025-01-26T20:25:26.834767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"concatenated_df.to_csv('Result.csv', index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T20:26:07.611157Z","iopub.execute_input":"2025-01-26T20:26:07.611627Z","iopub.status.idle":"2025-01-26T20:26:08.108284Z","shell.execute_reply.started":"2025-01-26T20:26:07.611587Z","shell.execute_reply":"2025-01-26T20:26:08.107134Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Thank You**","metadata":{}},{"cell_type":"code","source":"1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T22:16:16.397043Z","iopub.execute_input":"2025-01-26T22:16:16.397349Z","iopub.status.idle":"2025-01-26T22:16:16.404693Z","shell.execute_reply.started":"2025-01-26T22:16:16.397322Z","shell.execute_reply":"2025-01-26T22:16:16.403569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}